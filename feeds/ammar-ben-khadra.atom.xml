<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Formally Applied - Ammar Ben Khadra</title><link href="https://blog.formallyapplied.com/" rel="alternate"></link><link href="https://blog.formallyapplied.com/feeds/ammar-ben-khadra.atom.xml" rel="self"></link><id>https://blog.formallyapplied.com/</id><updated>2020-06-16T10:20:00+02:00</updated><entry><title>Visualizing program graphs using bcov</title><link href="https://blog.formallyapplied.com/2020/06/bcov-program-graphs/" rel="alternate"></link><published>2020-06-16T10:20:00+02:00</published><updated>2020-06-16T10:20:00+02:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2020-06-16:/2020/06/bcov-program-graphs/</id><summary type="html">&lt;p&gt;A couple of weeks ago, we released &lt;strong&gt;bcov&lt;/strong&gt;, a tool for efficient binary-level coverage analysis via static instrumentation. The tool supported only two operation modes, namely, patching and coverage reporting. Today, we add another operation mode that dumps various program graphs, like the CFG and dominator trees, for a given function in the binary. This article describes these graphs in more detail.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A couple of weeks ago, we released &lt;a href="https://github.com/abenkhadra/bcov"&gt;bcov&lt;/a&gt;, a tool for efficient binary-level coverage analysis via static instrumentation. The tool supported only two operation modes, namely, patching and coverage reporting. Today, we add another operation mode that dumps various program graphs, like the CFG and dominator trees, for a given function in the binary.&lt;/p&gt;
&lt;p&gt;This neat feature can be useful in its own right. For example, we heavily relied on visualizing these graphs at various stages during the development of &lt;strong&gt;bcov&lt;/strong&gt; itself. In this article, we first show how this mode can be used and then describe the generated program graphs.&lt;/p&gt;
&lt;h2&gt;Usage example&lt;/h2&gt;
&lt;p&gt;Consider the binary &lt;code&gt;gas&lt;/code&gt; which is distributed together with our &lt;a href="https://github.com/abenkhadra/bcov-artifacts/tree/master/sample-binaries"&gt;sample binaries&lt;/a&gt;.
For the sake of example, we will choose the function &lt;code&gt;string_prepend.part.5&lt;/code&gt;.
It is small but still contains an interesting number of basic blocks.
To dump its program graphs, you can simply issue,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bcov -m dump -i gas -f &lt;span class="s2"&gt;&amp;quot;string_prepend.part.5&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command should produce five DOT files in your current directory.
To visualize these files, we recommend the interactive viewer &lt;code&gt;xdot&lt;/code&gt;.
The raw DOT files are provided &lt;a href="/docs/prog_graphs.zip"&gt;here&lt;/a&gt; for convenience.
They are named after the address of the selected function as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;func_4e6070.cfg.dot. The CFG of the function.&lt;/li&gt;
&lt;li&gt;func_4e6070.rev.cfg.dot. Similar to the CFG but with all edges reversed.&lt;/li&gt;
&lt;li&gt;func_4e6070.pre.dom.dot. Predominator tree.        &lt;/li&gt;
&lt;li&gt;func_4e6070.post.dom.dot. Postdominator tree.   &lt;/li&gt;
&lt;li&gt;func_4e6070.sb.dom.dot. Superblock dominator graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first two graphs are probably familiar to you already. However, understanding the remaining graphs might require further reading. The definitive resource is the original &lt;a href="https://dl.acm.org/doi/10.1145/174675.175935"&gt;work&lt;/a&gt; that we implemented in &lt;strong&gt;bcov&lt;/strong&gt;.
However, the summary provided in section 3 of our &lt;a href="https://arxiv.org/pdf/2004.14191.pdf"&gt;paper&lt;/a&gt; should be sufficient.&lt;/p&gt;
&lt;h2&gt;Graphs described&lt;/h2&gt;
&lt;p&gt;We start with the CFG below. Each basic block is marked with its address (address of first instruction) and a unique identifier within the function.
Unconditional edges are blue colored. Conditional edges can be either green for the taken edge or red for the fall-through edge.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/func4e6070-cfg.png"
     alt="Control flow graph"
     width="400px"
     class="center"
     style="text-align:center; display: block; margin-left: auto; margin-right: auto;"&gt;
  &lt;figcaption&gt; CFG of function 'string_prepend.part.5'. Each basic block is marked with its address and a unique identifier (idx). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We move to the predominator tree. It is rooted in the virtual entry node. An edge between basic blocks A -&amp;gt; B means that A predominates B.
Simply put, B can not be visited without first visiting A. For example, BB&lt;sub&gt;8&lt;/sub&gt; (i.e., it has idx 8) must be visited before visiting BB&lt;sub&gt;9&lt;/sub&gt;.
This dominance relationship enables us to substantially reduce the required number of instrumentation probes since covering a particular BB implies that &lt;em&gt;all&lt;/em&gt; of its predecessors in the predominator tree are also covered.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/func4e6070-predom.png"
     alt="Predominator tree"
     width="670px"
     class="center"
     style="text-align:center; display: block; margin-left: auto; margin-right: auto;"&gt;
  &lt;figcaption&gt; Predominator tree of function 'string_prepend.part.5'. Basic blocks with idx 10 and 11 are the virtual entry and exit respectively. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The predominator tree is useful but we can still do better. First, we merge the predominator and postdominator trees in a dominator graph. Then, we identify the strongly-connected components (SCCs) in the latter graph to construct the superblock dominator graph (SDG). Each SCC represents a superblock that contains at least one basic block as shown below.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/func4e6070-sbdom.png"
     alt="Superblock dominator graph"
     width="230px"
     class="center"
     style="text-align:center; display: block; margin-left: auto; margin-right: auto;"&gt;
  &lt;figcaption&gt; Superblock dominator graph (SDG) of function 'string_prepend.part.5'. Every superblock has a unique identifier (idx) and contains the identifiers of its corresponding basic blocks. Similar to dominator trees, covering a node implies that all of its predecessors are also covered.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The instrumentation policies implemented in &lt;strong&gt;bcov&lt;/strong&gt; are based on the resulting SDG.
In the leaf-node policy, we only instrument the leaf nodes of the SDG (light-blue colored), namely, SB&lt;sub&gt;1&lt;/sub&gt; and SB&lt;sub&gt;2&lt;/sub&gt;.
SB&lt;sub&gt;1&lt;/sub&gt; consists of a single basic block, BB&lt;sub&gt;2&lt;/sub&gt;. However, SB&lt;sub&gt;2&lt;/sub&gt; offers more flexibility to choose between BB&lt;sub&gt;6&lt;/sub&gt; and BB&lt;sub&gt;7&lt;/sub&gt;.
We leverage this flexibility in &lt;strong&gt;bcov&lt;/strong&gt; to select the BB that incurs minimal overhead.&lt;/p&gt;
&lt;p&gt;A test suite that covers all leaf nodes would also achieve 100% code coverage.
Should you insist on a coverage target that ambitious, then the leaf-node policy is all that you need.
However, engineers in practice will have to make compromises since software testing is costly.
Additionally, a test suite achieving 100% code coverage is not necessarily more effective (i.e., detects more bugs) than a test suite that achieves, for example, only 80% coverage.&lt;/p&gt;
&lt;p&gt;In such settings, the leaf-node policy might not be adequate since it may lose coverage data.
The problem is that some superblocks might be visited in the CFG while bypassing all of their children in the SDG.
We refer to such superblocks as &lt;em&gt;critical&lt;/em&gt;. They are orange-colored by &lt;strong&gt;bcov&lt;/strong&gt; like SB&lt;sub&gt;3&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;The any-node policy solves this problem by instrumenting critical superblocks, in addition to the leaf superblocks instrumented in the leaf-node policy.
That is, &lt;strong&gt;bcov&lt;/strong&gt; would instrument one additional BB among the four in SB&lt;sub&gt;3&lt;/sub&gt;.
You can check that for &lt;em&gt;any&lt;/em&gt; input, it is now possible to precisely identify the set of covered basic blocks in function 'string_prepend.part.5'.
Such precision would require instrumenting only 3 BBs among a total of 10 BBs in this particular function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I'm interested in your take on this piece. Please reach out by opening an issue in this &lt;a href="https://github.com/abenkhadra/abenkhadra.github.io"&gt;repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;</content><category term="research"></category><category term="binary analysis"></category><category term="reverse engineering"></category></entry><entry><title>Function identification in stripped binaries revisited</title><link href="https://blog.formallyapplied.com/2020/05/function-identification/" rel="alternate"></link><published>2020-05-26T10:20:00+02:00</published><updated>2020-05-26T10:20:00+02:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2020-05-26:/2020/05/function-identification/</id><summary type="html">&lt;p&gt;In this article, I revisit the problem of function identification  with two goals in mind. First, overviewing the state of the art in a systematic way. Second, discussion and quantification of the (often overlooked) role that call frame information can play as a source of function definitions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The notion of a function is important for the modularity and scalability of program analyses. For some applications, like control-flow integrity, functions represent an indispensable abstraction. Functions are well-defined in the source code and easy to identify in non-stripped binaries. However, identifying functions in stripped binaries can be challenging.&lt;/p&gt;
&lt;p&gt;This motivated several works, including our CASES'16 &lt;a href="https://doi.org/10.1145/2968455.2968505"&gt;paper&lt;/a&gt;, to tackle this problem. In this article, I revisit this problem with two goals in mind. First, overviewing the state of the art in a systematic way. Second, discussion and quantification of the (often overlooked) role that call frame information can play as a source of function definitions.&lt;/p&gt;
&lt;h2&gt;What is a binary-level function?&lt;/h2&gt;
&lt;p&gt;Before delving into function identification techniques, the notion of a binary-level function needs to be defined in first place.
Functions are well-defined constructs in the source code.
For example, a couple of curly braces can clearly mark the scope of a function in C/C++.
However, things are not that straightforward when we move to binary-level
functions. Compiler optimizations such as function inlining and splitting
can substantially alter how functions are laid out in the binary.
This caused a discrepancy in the research community on how functions are defined.&lt;/p&gt;
&lt;p&gt;Generally, binary-level functions are  defined based on two models;
they are the &lt;em&gt;contiguous&lt;/em&gt; and &lt;em&gt;chunk&lt;/em&gt; models. In the former, a function
simply occupies a single contiguous code region. The latter model, on the other hand, aims
for more generality by allowing a function to span multiple chunks. Each
chunk in turn is a contiguous code region. The chunk model might be
motivated by a need to maintain the mapping between a single source-level
function and its corresponding chunks in the binary, which can be many. As a
result, a single chunk might be shared between several functions. This model
is adopted in tools like &lt;a href="https://dl.acm.org/doi/10.1145/2931037.2931047"&gt;Dyninst&lt;/a&gt; and &lt;a href="https://doi.org/10.1109/EuroSP.2017.11"&gt;nucleus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The contiguous model has its adopters, too. In "&lt;a href="https://arxiv.org/abs/2004.14191"&gt;efficient binary-level coverage analysis&lt;/a&gt;" (accepted to ESEC/FSE 2020), we show that despite its irresistible simplicity, the contiguous model is consistent with the functions
defined in the symbol table in our large corpus of x86-64 binaries.
Here, consistency means that we observed that two key assumptions generally hold. First, most basic blocks in the function's region are reachable from its entry (reachability). Second, functions do not target arbitrary basic blocks inside each other, rather only their respective entries (encapsulation).  Reachability and encapsulation are important properties for the modularity of program analyses that build on functions as a core abstraction.&lt;/p&gt;
&lt;p&gt;There is a twist, however. We allowed a single function to have multiple entry points. This was necessary for the accurate treatment of constructs such as C++ exceptions. In this way, we leveraged the simplicity of the contiguous model without loss in generality. That said, our discussion will only focus on this model. It defines a function by its boundaries; start and end addresses. This definition can be augmented with a set of entry and exit points which determine where control-flow can enter or leave the function respectively.&lt;/p&gt;
&lt;h2&gt;Haven’t we solved this problem yet?&lt;/h2&gt;
&lt;p&gt;Identifying functions in non-stripped binaries is straightforward; just read the
symbol table. Therefore, our attention is focused on the more interesting case of identifying functions in stripped binaries. The techniques proposed for addressing this problem can be broadly categorized to manual pattern matching, machine learning, and CFG-based techniques.&lt;/p&gt;
&lt;p&gt;Functions prologues and epilogues often contain instructions for stack setup and
tear down respectively.
Manually writing heuristics for detecting such patterns is not difficult.
For example, a heuristic that looks for the &lt;code&gt;push&lt;/code&gt;es can provide a rough approximation of function starts. However, heuristics being heuristics, they can only take us so far.&lt;/p&gt;
&lt;p&gt;Pattern matching can be automated using machine learning. This research theme has been considered in several works. The beginning was with &lt;a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-127.pdf"&gt;Rosenblum et al.&lt;/a&gt; who leveraged conditional random fields. Other notable works include &lt;a href="https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-bao.pdf"&gt;ByteWeight&lt;/a&gt; which relies on weighted prefix trees, and &lt;a href="https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-shin.pdf"&gt;Shin et al.&lt;/a&gt; who decided to go for recurrent neural networks instead.&lt;/p&gt;
&lt;p&gt;The effectiveness of a machine learning technique is measured by its ability to generalize beyond its training set. However, modern compiler optimizations are diverse and sophisticated. Additionally, optimizations are conducted at all levels. From a single compilation unit up to whole-program link-time optimization. Add to this post-link optimization which is witnessing a recent revival like Facebook's &lt;a href="https://dl.acm.org/doi/10.5555/3314872.3314876"&gt;BOLT&lt;/a&gt;. All of this makes function identification by learning byte-level features quite challenging to generalize.&lt;/p&gt;
&lt;p&gt;This leaves us with CFG-based identification techniques. In our CASES'16 &lt;a href="https://doi.org/10.1145/2968455.2968505"&gt;paper&lt;/a&gt;, we proposed to leverage the control flow graph (CFG) to identify functions. Basically, we partition a global CFG, which is recovered after disassembly, to a call graph connecting multiple function-level CFGs. It seems that the time was ripe for such ideas. In the following year not one, but two similar CFG-based proposals were published. They are   &lt;a href="https://doi.org/10.1109/EuroSP.2017.11"&gt;Nucleus&lt;/a&gt; which appeared in Euro S&amp;amp;P'17 and &lt;a href="https://doi.org/10.1109/DSN.2017.29"&gt;Function Interface Analysis&lt;/a&gt; in DSN'17. In the latter, the authors incorporated additional data-flow analyses.&lt;/p&gt;
&lt;p&gt;What all of these proposals have in common are the following key mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Call target analysis. Collects the targets of direct calls as a first step to identify function entries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CFG traversal. Traverse a function's CFG to determine its end address. This is the end of the last basic block reachable from the entry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tail-call target analysis. Direct jumps targeting regions outside the current function are considered tail-calls which are used to identify the entries of more functions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first mechanism is independent and can be done in one pass. The remaining two can be iteratively applied until a fix point is reached, i.e., no more function entries to discover.&lt;/p&gt;
&lt;p&gt;CFG-based identification is practical and highly accurate with consistent results across all of the aforementioned works. These results motivated &lt;a href="https://binary.ninja/2017/11/06/architecture-agnostic-function-detection-in-binaries.html"&gt;binary ninja&lt;/a&gt; to quickly adopt it. So can we say that the problem is now solved? I would say that it is solved to a significant extent, but certainly not entirely.&lt;/p&gt;
&lt;p&gt;The main issue here is that we rely on the ability to traverse the CFG in the second step. However, the recovered CFG might not be precise enough for our purpose. I can provide some concrete examples which might prove challenging:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Adjacent functions. Consider two adjacent functions A and B. The latter is either indirectly called or not called at all. A tail-call from A to B is indistinguishable in this case from a usual intra-procedural jump. Hence, both functions will be merged in a single function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non-return functions. Should a function end with a call to an undetected non-return function, then it is possible to continue the CFG traversal stepping into the following function. Some non-return functions are well-known like &lt;code&gt;exit&lt;/code&gt; and &lt;code&gt;abort&lt;/code&gt;. However, detecting non-return functions generally requires analyzing the call-graph. However, building the call-graph depends in turn on having function definitions already. This leads to a cyclical dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inter-procedural conditionals. Compilers never fail to surprise especially when it comes to undefined behavior. Consider the implementation of &lt;code&gt;llvm_unreachable&lt;/code&gt; (see &lt;a href="https://llvm.org/docs/ProgrammersManual.html"&gt;LLVM programmer's manual&lt;/a&gt;). In a debug build, it will print an error message and terminate with a call to &lt;code&gt;abort&lt;/code&gt;. All good so far. In a release build, however, it will hint to the compiler that there is no need to generate code since reaching such a statement is undefined behavior. In this case, the compiler might decide to emit a conditional jump at the end of the function which if not satisfied, will branch into the next function, whatever that is. We do not have access to such hints in the binary which can lead us to continue the CFG traversal.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, this list is by no means exhaustive. Also, issues can coincide to further complicate matters. For example, having too many indirect calls in the binary which might reduce the quality of the call target analysis in the first step. Nevertheless, I think that, in practice, it is safe to expect CFG-based identification to be able to recover well over 90% of the functions.&lt;/p&gt;
&lt;h2&gt;Are stripped binaries really stripped?&lt;/h2&gt;
&lt;p&gt;We looked at function identification assuming that if the symbol table is stripped, then we have no alternative but to analyze the binary itself. This assumption might have been valid in the old days of &lt;code&gt;ebp&lt;/code&gt; based stack unwinding in x86. However, it is generally not valid anymore in x86-64, or perhaps other ISAs for that matter.&lt;/p&gt;
&lt;p&gt;Nowadays, call frame information (CFI) records are embedded in the majority of commercial off-the-shelf binaries. They are the basis of the modern stack unwinding mechanism used in C++ exception handling. However, there are several good reasons for including CFI records in C programs as well like generating backtraces. This SO &lt;a href="https://stackoverflow.com/a/26302715"&gt;answer&lt;/a&gt; provides more examples.&lt;/p&gt;
&lt;p&gt;CFI records can be found in the &lt;code&gt;eh_frame&lt;/code&gt; section which is part of the loadable binary image. You can dump them using something like,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ readelf --debug-dump&lt;span class="o"&gt;=&lt;/span&gt;frames /bin/ls
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output shows some Common Information Entry (CIE) records encapsulating many Frame Description Entry (FDE) records like the one in this example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;00000048 0000000000000024 0000001c FDE cie=00000030 pc=0000000000003770..0000000000003e70&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_def_cfa_offset: 16&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_advance_loc: 6 to 0000000000003776&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_def_cfa_offset: 24&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_advance_loc: 10 to 0000000000003780&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_def_cfa_expression (DW_OP_breg7 (rsp): 8; DW_OP_breg16 (rip): 0; DW_OP_lit15; DW_OP_and; DW_OP_lit11; DW_OP_ge; DW_OP_lit3; DW_OP_shl; DW_OP_plus)&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_nop&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_nop&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_nop&lt;/span&gt;
&lt;span class="err"&gt;  DW_CFA_nop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This FDE record defines a function in the region from &lt;code&gt;pc=0000000000003770&lt;/code&gt; to &lt;code&gt;0000000000003e70&lt;/code&gt; which are start and end addresses respectively. This is the only bit of information of concern to us. The cryptic expressions prefixed with &lt;code&gt;DW_&lt;/code&gt; can be used by a debugger like &lt;code&gt;gdb&lt;/code&gt; to restore register values during stack unwinding.&lt;/p&gt;
&lt;p&gt;I move now to do some hands-on experimentation with CFI records. My goal is to first quantify the existence of the &lt;code&gt;.eh_frame&lt;/code&gt; section in practice. More important, to see to what extent the functions defined in &lt;code&gt;.eh_frame&lt;/code&gt; section are complete. After all, an &lt;code&gt;.eh_frame&lt;/code&gt; with only a handful of function definitions is not that useful. Note the developers might deliberately omit CFI records of some functions to save space.&lt;/p&gt;
&lt;p&gt;For this experiment which was conducted on a typical Ubuntu 18.04 installation, I searched all default executable and library paths, like &lt;code&gt;/usr/bin&lt;/code&gt; and &lt;code&gt;/lib&lt;/code&gt;, looking for ELF binaries.
I managed to find 5,538 ELF binaries in total. An &lt;code&gt;.eh_frame&lt;/code&gt; section did exist in &lt;em&gt;all&lt;/em&gt; of those binaries. Having established the existence of function definitions, the question is now to what extent are they complete?&lt;/p&gt;
&lt;p&gt;In order to measure the completeness of functions defined in the &lt;code&gt;.eh_frame&lt;/code&gt;, I sum their sizes and then divide that sum by the size of the &lt;code&gt;.text&lt;/code&gt; section. I exclude the few functions located in other sections. The result is a metric that I will refer to as function coverage ratio. Intuitively, a coverage ratio of 100% means that all code in &lt;code&gt;.text&lt;/code&gt; lies within functions defined already, i.e., more functions can not exist. However, data bytes are often embedded in the &lt;code&gt;.text&lt;/code&gt; section. For example, padding bytes which are used to align function starts. Therefore, a ratio greater than 95% can be deemed to achieve full coverage.&lt;/p&gt;
&lt;p&gt;The distribution of the coverage ratio for this dataset is depicted below.
About 400 binaries have a coverage ratio of less than 5%. Many of which have empty &lt;code&gt;.eh_frame&lt;/code&gt; sections with no FDEs at all. On the other hand, we have a comfortable majority of about 4,000 binaries (72% of total) with coverage ratio of more than 90%.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/funct-ident1.png"
     alt="Distribution of all binaries"
     width="450px"
     class="center"
     style="text-align:center; display: block; margin-left: auto; margin-right: auto;"&gt;
  &lt;figcaption&gt;Distribution of coverage ratio for functions defined in the .eh_frame section. The dataset consists of 5,538 ELF binaries found in the default paths of a typical Ubuntu 18.04 installation.  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;My dataset is somewhat comprehensive but not necessarily representative of the binaries that are "relevant". In order to obtain a more representative dataset, I omitted system binaries found in the paths &lt;code&gt;/usr/lib/syslinux&lt;/code&gt;, &lt;code&gt;/usr/lib/klibc&lt;/code&gt;, and &lt;code&gt;/usr/lib/debug/&lt;/code&gt;. I also omitted smaller binaries which have a &lt;code&gt;.text&lt;/code&gt; section of size less than 4KB. This results in a dataset of 3,574 binaries that I deemed to be "relevant”.&lt;/p&gt;
&lt;p&gt;The coverage ratio distribution is now quite different as shown in the figure below. The overall trend is clear. CFI records are mostly complete in larger user-space binaries. Admittedly, relevance here is a subjective criteria.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/funct-ident2.png"
     alt="Distributions of relevant binaries"
     width="450px"
     class="center"
     style="text-align:center; display: block; margin-left: auto; margin-right: auto;"&gt;
  &lt;figcaption&gt;Distribution of coverage ratio for functions defined in the .eh_frame section. This is a subset of the original dataset consisting of 3,574 ELF binaries that are deemed to be "relevant". &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I will simply conclude with a short, cookbook style, recommendation. Look for CFI records, if available, and measure their function coverage ratio. If the ratio is insufficient for your application, then add to the mix one (or more) of the CFG-based mechanisms. Be cautious about CFG imprecision as it might introduce issues during traversal.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I'm interested in your take on this piece. Please reach out by opening an issue in this &lt;a href="https://github.com/abenkhadra/abenkhadra.github.io"&gt;repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;</content><category term="research"></category><category term="binary analysis"></category><category term="reverse engineering"></category></entry><entry><title>An introduction to approximate computing</title><link href="https://blog.formallyapplied.com/2017/11/approx-intro/" rel="alternate"></link><published>2017-11-07T10:20:00+01:00</published><updated>2017-11-07T10:20:00+01:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2017-11-07:/2017/11/approx-intro/</id><summary type="html">&lt;p class="first last"&gt;Approximate computing is a wide spectrum of techniques that relax
accuracy of computations in order to improve performance, energy, and/or other
metric of merit. In this article, I'll try to provide a structured introduction
to this research area.&lt;/p&gt;
</summary><content type="html">&lt;blockquote class="pull-quote"&gt;
&lt;p&gt;12 Dec 2017. This article is now available on &lt;a class="reference external" href="https://arxiv.org/abs/1711.06115"&gt;arXiv&lt;/a&gt; with more updates.&lt;/p&gt;
&lt;p&gt;15 Nov 2017. General text improvements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Approximate Computing (AC) is a wide spectrum of techniques that relax
the accuracy of computation in order to improve performance, energy, and/or another
metric of merit.
AC exploits the fact that several important applications,
like machine learning and multimedia processing, do not require precise results to be useful.&lt;/p&gt;
&lt;p&gt;For instance, we can use a lower resolution image encoder
in applications where high-quality images are not necessary.
In a data center, this may lead to large savings in
the amount of required processing, storage, and communication bandwidth.&lt;/p&gt;
&lt;p&gt;Research interest in AC has been growing in recent years.
I refer here to two recent surveys &lt;a class="footnote-reference" href="#id17" id="id1"&gt;[1]&lt;/a&gt; &lt;a class="footnote-reference" href="#id18" id="id2"&gt;[2]&lt;/a&gt; for a comprehensive treatment.
In this article, I'll try to provide an introduction to this research field.&lt;/p&gt;
&lt;p&gt;Basically, my discussion starts from the absolute beginning by motivating
AC and discussing its research scope.
Then, I discuss key concepts based on a proposed taxonomy.
Finally, I further elaborate on nondeterministic AC, an AC category with unique opportunities as well as challenges.&lt;/p&gt;
&lt;p&gt;Graduate students will, hopefully, find this introduction useful
to catch up with recent developments.&lt;/p&gt;
&lt;p&gt;Note that some of the ideas discussed here are based on a poster I presented at an AC workshop.
The workshop was a satellite event to &lt;a class="reference external" href="http://esweek.org/"&gt;ESWEEK'16&lt;/a&gt;.
Our extended  &lt;a class="reference external" href="/docs/approx16.pdf"&gt;abstract&lt;/a&gt; highlighted some AC challenges and opportunities in general.
There, we argued that nondeterministic AC faces a fundamental
control-flow &lt;em&gt;wall&lt;/em&gt; which is bad news! The good news, however, is that there are
still enough opportunities in deterministic AC to keep practitioners busy for the foreseeable future.&lt;/p&gt;
&lt;p&gt;I shall elaborate on those issues and more in the following.&lt;/p&gt;
&lt;div class="section" id="motivation"&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;So let's start by trying to answer the &lt;em&gt;why&lt;/em&gt; and &lt;em&gt;what&lt;/em&gt; questions for AC.
Our concrete questions in this regard are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What motivates the recent academic interest in AC? In other words why do we
need to care about AC more than before?&lt;/li&gt;
&lt;li&gt;What approximate computing really is? Note that AC is used in practice
for decades already, so what makes recent academic proposals different?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, research in AC can be motivated by two key concerns, namely, power and reliability.&lt;/p&gt;
&lt;p&gt;Nowadays, the majority of our computations are being done either on mobile devices or in large data centers (think of cloud computing).
Both platforms are sensitive to power consumption.
That is, it would be nice if we can extend the operation time of smartphones and other battery powered devices
before the next recharge.&lt;/p&gt;
&lt;p&gt;Also, and perhaps more importantly, the energy costs incurred
on data centers need to be reduced as much as possible. Note that power is one of (if not the) major
operational cost of running a data center. To this end, algorithmic optimizations,
dynamic run-time adaptation, and various types of hardware accelerators are used in practice.&lt;/p&gt;
&lt;p&gt;Vector processors, FPGAs, GPUs, and even ASICs (like Google's recent &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tensor_processing_unit"&gt;TPU&lt;/a&gt;)
are all being deployed in an orchestrated effort to optimize performance and reduce power consumption.&lt;/p&gt;
&lt;p&gt;In this regard, the question that the AC community is trying to address is;
can we exploit the inherit approximate results of some application to gain more power savings?&lt;/p&gt;
&lt;p&gt;We move now to reliability concerns to further motivate AC.
The semiconductor industry is aggressively improving it's production processes to keep
pace with the venerable Moore's law (or maybe a bit slower version of it in recent years).
Microchips produced with &lt;a class="reference external" href="https://en.wikipedia.org/wiki/10_nanometer"&gt;10 nanometer&lt;/a&gt;
processes are already shipping to production.
Moving toward 7 nanometers and beyond is expected within the next 5 years according
to the &lt;a class="reference external" href="http://www.itrs2.net/"&gt;ITRS&lt;/a&gt; roadmap.&lt;/p&gt;
&lt;p&gt;Such nanometer regimes are expected to cause a twofold problem.
First, transistors can be more susceptible to faults (both temporary and permanent ones).
For example, cosmic radiation can more easily cause a glitch in data stored in a Flip-Flop.
Consequently, more investment might be necessary in hardware fault avoidance, detection, and recovery.&lt;/p&gt;
&lt;p&gt;Second, feature variability between microchips or even across the same microchip
can also increase. This means that design margins need to be more pessimistic
to account for such large variability. That is, manufacturers have to allow for a wider margin
for the supply voltage, frequency, and other operational parameters in order to
keep the chip reliable while maintaining an economical yield.&lt;/p&gt;
&lt;p&gt;In response to these reliability challenges, the AC community is investigating
the potential of using  narrower margins to operate microchips.
However, hardware faults might occasionally propagate to software in this case.
Therefore, the research goal is to ensure that such faults do not cause program outputs to diverge too much from the ideal outputs.&lt;/p&gt;
&lt;p&gt;Such schemes can allow chip manufacturers to relax their investments in maintaining hardware reliability.
More performance and power saving opportunities can be harnessed by moving to best-effort hardware instead &lt;a class="footnote-reference" href="#id19" id="id3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="definition-and-scope"&gt;
&lt;h2&gt;Definition and scope&lt;/h2&gt;
&lt;p&gt;So far, we motivated the need for approximate computing.
We move now to our second question; What is approximate computing in the first place?&lt;/p&gt;
&lt;p&gt;Let's come back to our image encoding example. We know that applying any lossy compression
algorithm (JPEG for example) to a raw image will result in an approximate image.
Such compression often comes (by design) with little human perceptible loss in image quality.
Also, image encoders usually have tunable algorithmic &lt;em&gt;knobs&lt;/em&gt;, like compression level,
to trade off image size with its quality.&lt;/p&gt;
&lt;p&gt;Therefore, do not we already know how to do AC on images?
Actually, instances of AC are by no means limited to image processing.
AC is visible in many domains from wireless communication to control systems and beyond.
In fact, one can argue that every computing system ever built did require balancing trade-offs
between cost and quality of results.
This is what engineering is about after all.&lt;/p&gt;
&lt;p&gt;Indeed, approximate computing is already a stable tool in the engineering toolbox.
However, it's usually applied &lt;em&gt;manually&lt;/em&gt; leveraging domain knowledge and past experience.
The main goal of recent AC research, I think, is to introduce &lt;em&gt;automation&lt;/em&gt; to the approximation process.
That is, the research goal is to (semi-) automatically derive/synthesize more efficient computing systems
which produce approximate results that are good enough.&lt;/p&gt;
&lt;p&gt;I shall elaborate on this point based on Figure (1).
Consider for example that you have been given a computing system with a well-specified functionality.
Such system can consist of software, hardware, or a combination of both.
Now, your task is to optimize this system to improve its performance.
How would you typically go about this task?&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="manual vs. automatic approximate computing" src="/images/approx-fig01.png" /&gt;
&lt;p class="caption"&gt;Figure (1): AC can be applied (a) manually in the usual profile/optimize cycle,
or (b) automatically via an approximation method which may require providing an error quality specification.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Ideally, you start by collecting &lt;em&gt;typical&lt;/em&gt; inputs which represent what you expect
the system to handle in the real world. Then, based on those inputs, you attempt to profile
the system to identify hot regions where the system spends most of the time.
After that, the serious optimization work begins which might involve several system layers.&lt;/p&gt;
&lt;p&gt;For example, in a software program, you will often need to modify the algorithms and data structures used.
You might also go all the way down to the nitty-gritty details of improving cache alignment or
'stealing' unused bits in some data structures for other purposes.&lt;/p&gt;
&lt;p&gt;This profile/optimize cycle continues until you either meet your performance target
or you think that you have reached the point of diminishing returns.
This optimization process is generally applicable to any computing system.
However, with a system that can tolerate &lt;em&gt;controllable&lt;/em&gt; deviations from its original outputs,
you can go a bit further in your optimization.&lt;/p&gt;
&lt;p&gt;Basically, AC is about this last mile in optimization.
The research goal is to investigate &lt;strong&gt;automatic&lt;/strong&gt;, &lt;strong&gt;principled&lt;/strong&gt;, and ideally &lt;strong&gt;generic&lt;/strong&gt; techniques
to gain more efficiency by relaxing the exactness of outputs.
The need for automation is obvious since manual approximation techniques can simply be regarded as 'business as usual'
i.e., without clear improvement over the state of practice.&lt;/p&gt;
&lt;p&gt;Furthermore, AC needs to guarantee, in a principled way, that the expected output errors
will remain 'acceptable' in the field.
That is, computing systems already struggle with implementation bugs. Therefore, it's difficult to adopt
an AC technique that can introduce more bugs in the form of arbitrary outputs.&lt;/p&gt;
&lt;p&gt;We come to the third criterion which is generality.
I consider a technique to be generic if it is applicable to a wide spectrum of domains of interest to AC.
For example, loop invariant &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Loop-invariant_code_motion"&gt;code motion&lt;/a&gt;
is a generic compiler optimization that applies to virtually any program from scientific simulations
to &lt;a class="reference external" href="https://en.wikipedia.org/wiki/High-level_synthesis"&gt;high-level synthesis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Generality, however, is more challenging to achieve in AC compared to the 'safe' optimizations used in compilers.
A &lt;em&gt;local&lt;/em&gt; AC optimization might introduce errors which are difficult to reason about when combined
with other &lt;em&gt;local&lt;/em&gt; optimizations.
Note that the combined error observed on the &lt;em&gt;global&lt;/em&gt; outputs might be composed of several local errors.&lt;/p&gt;
&lt;p&gt;Actually, I'd argue that it's not feasible to target, to a satisfactory level, all three criteria.
In other words, better automation and principled guarantees require compromising on generality.
This can be achieved by embedding domain-specific knowledge in the AC technique.
This seems to be a reasonable thing to do given the diversity of domains where AC is applicable.&lt;/p&gt;
&lt;p&gt;A prime advantage of compromising on generality is that end users won't need to explicitly
provide error quality specification, see Figure (1).
Metrics of acceptable errors will be based on the specific domain.
For instance, &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Generalization_error"&gt;generalization error&lt;/a&gt; in machine learning and
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"&gt;PSNR&lt;/a&gt; in image compression.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-taxonomy-of-approximate-computing"&gt;
&lt;h2&gt;A taxonomy of approximate computing&lt;/h2&gt;
&lt;p&gt;The literature on approximate computing is large and growing.
Also, it covers the entire system stack from high-level algorithms down to individual hardware circuits.
It's difficult to make sense of all of these developments without introducing some sort of structure.
In this section, I'll attempt such structuring based on the taxonomy depicted in Figure (2).
Also, selected pointers to the relevant literature will be highlighted.&lt;/p&gt;
&lt;p&gt;Basically, my hypothesis is that we can map any individual AC technique to a point in a
three-dimensional space. The considered axes represent the approximation level,
required run-time support, and behavior determinism respectively.
Of course, there are papers on AC that combine several techniques in one proposal.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="approximate computing taxonomy" src="/images/approx-fig02.png" /&gt;
&lt;p class="caption"&gt;Figure (2): Proposed AC taxonomy. Expected cost of targeting a design point
rises as we move away from the center.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The reader might be wondering why hardware circuits have been placed higher up while
the algorithm level is at the bottom? The reason is simply the expected &lt;em&gt;cost&lt;/em&gt; of targeting
such a design point. In other words, I expect the implementation cost to increase
as one explores design points further away from the center.&lt;/p&gt;
&lt;p&gt;For instance, a system that involves dynamic run-time adaption, e.g., for error quality monitoring,
is more complex, and thus more costly, to build and maintain compared to a static system.
However, run-time adaptation might provide sufficient benefits to amortize the higher cost if designed with care.&lt;/p&gt;
&lt;p&gt;Now, we move to the determinism axis.
My classification is based on the usual &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Nondeterministic_algorithm"&gt;determinism&lt;/a&gt; property.
That is, an algorithm that returns the same output repeatedly given the same input is &lt;em&gt;deterministic&lt;/em&gt;.
Nondeterministic algorithms do not exhibit such output repeatability.&lt;/p&gt;
&lt;p&gt;I further classify nondeterministic algorithms to &lt;em&gt;partially&lt;/em&gt; and &lt;em&gt;fully&lt;/em&gt; nondeterministic.
These categories are based on the sources of nondeterminism in the algorithm and bug reproducibility.
Nondeterministic AC will be discussed in more detail in the next section.&lt;/p&gt;
&lt;p&gt;The axis of approximation level in Figure (2) has been (roughly) divided into 4 categories.
At algorithm level, a given algorithm is kept intact.
To implement approximation, one has to manipulate either the inputs or algorithm configurations
(knobs or &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)"&gt;hyperparameters&lt;/a&gt;).
An example of the former can be found in ApproxHadoop &lt;a class="footnote-reference" href="#id20" id="id4"&gt;[4]&lt;/a&gt; where the authors utilized a statistical input
sampling scheme in order to derive approximate results.&lt;/p&gt;
&lt;p&gt;In comparison, one can leave the inputs and modify the hyperparameters instead.
Approximation via hyperparameter
optimization is a well-established research theme in machine learning. It's known there as 'learning to learn'
or 'meta-learning'. I won't elaborate on this here and refer the reader to &lt;a class="footnote-reference" href="#id21" id="id5"&gt;[5]&lt;/a&gt;
simply because the paper title seems 'meta' enough.&lt;/p&gt;
&lt;p&gt;Also noteworthy in the algorithm category is Capri &lt;a class="footnote-reference" href="#id22" id="id6"&gt;[6]&lt;/a&gt;.
There, the authors formulate knob tuning as a constrained optimization problem.
To solve this problem, their proposed system learns cost and error models using bayesian networks.&lt;/p&gt;
&lt;p&gt;Let's move now to application level approximation where we have to
modify things &lt;em&gt;inside&lt;/em&gt; the original algorithm. Compare this to the previous level
where the original algorithm was an untouchable black-box.
A good example here is loop perforation &lt;a class="footnote-reference" href="#id23" id="id7"&gt;[7]&lt;/a&gt;.
There, the authors identified certain loop patterns and propose techniques to automatically skip
loop iterations.&lt;/p&gt;
&lt;p&gt;For approximation at the architecture level, I refer to Quora &lt;a class="footnote-reference" href="#id24" id="id8"&gt;[8]&lt;/a&gt;.
Basically, the authors propose to extend the ISA of vector processors such
that computation quality can be specified in the instruction set.
Error precision is deterministically bound for each instruction.&lt;/p&gt;
&lt;p&gt;Finally, there is approximation in hardware circuits.
There are several proposals in the literature for approximate arithmetic units like
adders and multipliers. They can be generally classified to deterministic
(with reduced precision) and nondeterministic. Units of the latter type work most of
the time as expected. However, they can occasionally produce arbitrary outputs.&lt;/p&gt;
&lt;p&gt;The approximate circuits previously discussed are mostly designed manually for their specific purpose.
In contrast, the authors of SALSA &lt;a class="footnote-reference" href="#id25" id="id9"&gt;[9]&lt;/a&gt; approach hardware approximation from a different angle.
They propose a general technique to &lt;em&gt;automatically&lt;/em&gt; synthesize approximate circuits given golden RTL model and quality specifications.&lt;/p&gt;
&lt;p&gt;I conclude this section by discussing how cost is expected to increase as we go from algorithm level to circuit
level. Given a (correct) algorithm that exposes some configurable knobs. Adapting such algorithm
to different settings is relatively cheap. Also, it can be highly automated based on established meta-
optimization literature as can be found in meta-learning.&lt;/p&gt;
&lt;p&gt;However, things get more challenging if we were to approximate outputs based on the internal workings of an algorithm.
Generally, this application level approximation requires asking users for annotations or assumptions on expected inputs.
Consequently, there is a smaller opportunity for automation and more difficulty in guaranteeing error quality.&lt;/p&gt;
&lt;p&gt;The expected cost gets even higher at architecture level were several stakeholders might be affected.
Compiler engineers, operating systems developers, and hardware architects all need to be either
directly involved or at least aware of the approximation intended by the original algorithm developers.
A proposed AC technique should demonstrate a serious value across the board to convince all of these
people to get involved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="on-nondeterministic-approximate-computing"&gt;
&lt;h2&gt;On nondeterministic approximate computing&lt;/h2&gt;
&lt;p&gt;Let's begin this section with a definition;
An algorithm is considered to be nondeterministic if its output can be different for the same input.
In a partially nondeterministic algorithm, nondeterminism sources can be feasibly accounted for a priori.
Otherwise, the algorithm is considered fully nondeterministic.&lt;/p&gt;
&lt;p&gt;Simulated annealing is an example of a partially nondeterministic algorithm where
picking the next step is based on a random choice.
Despite this, its control-flow behavior remain predictable which makes it relatively
easy to reproduce implementation bugs with repeated runs.
In contrast, the followed control-flow path can differ in 'unexpected' ways
in the case of fully nondeterministic algorithms.&lt;/p&gt;
&lt;p&gt;A nondeterministic AC technique introduces (or increases) nondeterminism in a given
algorithm. This can be realized in several ways.
In ApproxHadoop, the authors proposed random task dropping to gain more efficiency. Also, authors of SAGE &lt;a class="footnote-reference" href="#id26" id="id10"&gt;[10]&lt;/a&gt;
proposed skipping atomic primitives to gain performance at the expense of
exposing the algorithm to race conditions.&lt;/p&gt;
&lt;p&gt;Nondeterministic AC introduced by unreliable hardware has
received special attention from the research community. This is motivated
by the potential efficiency gains discussed already.
I'll focus in the following on nondeterministic AC that is realized by unreliable hardware.
Note that an algorithm is, typically, fully nondeterministic in this case.&lt;/p&gt;
&lt;p&gt;There several possibilities to implement hardware-based nondeterministic AC.
DRAM cells need a periodic refresh to retain their data which consumes energy.
Equipping DRAMs with 'selective' no-refresh mechanisms saves energy but risks occasional bit errors.&lt;/p&gt;
&lt;p&gt;Similarly, dynamically adapting bus compression and error detection mechanisms can provide  significant
gains in the communication between main memory and processing cores.&lt;/p&gt;
&lt;p&gt;Additionally, there are efficiency opportunities in allowing processing cores
themselves to provide a best-effort rather than a reliable service.
In this setting, hardware engineers may &lt;em&gt;optimistically&lt;/em&gt; invest in reliability
mechanisms.&lt;/p&gt;
&lt;p&gt;However, software programmers need, in turn, to be aware of hardware unreliability and
invest in fault management schemes suitable for their particular needs.
Relax &lt;a class="footnote-reference" href="#id27" id="id11"&gt;[11]&lt;/a&gt; provide a good example of such an arrangement.&lt;/p&gt;
&lt;p&gt;Also, CLEAR &lt;a class="footnote-reference" href="#id28" id="id12"&gt;[12]&lt;/a&gt; provides an interesting design-space exploration
of reliability against soft-errors across the entire system stack.&lt;/p&gt;
&lt;p&gt;Despite the extensive research in reliability in general and more recently
in AC. The problem of running software &lt;em&gt;reliably&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt;
on unreliable hardware is far from solved.
Beside the cost factor mentioned in the previous section,
there are still major interdisciplinary problems to be addressed.&lt;/p&gt;
&lt;p&gt;First, there is the abstraction problem of the hardware/software interface.
Extending the ISA abstraction makes sense given its ubiquity.
For example, each ISA instruction might be extended with probability specification.
This probability quantifies how many times this nondeterministic instruction is expected to
supply correct results (I'm becoming frequentist here).&lt;/p&gt;
&lt;p&gt;However, microchip designs nowadays are complex possibly comprising tens of IP modules
from several IP providers. It's difficult for a microchip manufacturer to derive such
probabilities per individual instruction.
Even where such derivation is possible,
microchip manufacturers would be reluctant to guarantee such probabilities
to their customers. Maintaining such guarantees through the entire product lifetime
would prove costly.&lt;/p&gt;
&lt;p&gt;Further, I think that it's still not clear how much value can this instruction-level abstraction
provide to hardware as well as software engineers.
Consequently, the question of finding suitable and generic abstractions between software and unreliable hardware is still open.&lt;/p&gt;
&lt;p&gt;Second, there is the correctness challenge.
Can we establish that a software running on nondeterministic hardware is correct
or maybe &lt;a class="reference external" href="https://www.cs.cornell.edu/~asampson/blog/probablycorrect.html"&gt;probably correct&lt;/a&gt;?
A short answer is probably no!&lt;/p&gt;
&lt;p&gt;There are several reasons for this correctness challenge.
Statistical correctness requires sampling from (joint) probability distributions of inputs
which are typically not available a priori.
Also, software functions are generally noncontinuous and nonlinear in the mathematical sense.
This makes them good at hiding unexpected behaviors in the corners.&lt;/p&gt;
&lt;p&gt;More importantly, algorithms process inputs in deterministic steps based on a
specific control-flow. Nondeterministic hardware might affect control-flow decisions
causing the algorithm to immediately fail, or worse, proceed and produce arbitrary outputs.&lt;/p&gt;
&lt;p&gt;Note that the focus of this discussion is on the setting of a single computing node.
Fault-tolerance in distributed systems consisting of many nodes is quite different.
Although, the latter might be affected if individual nodes continue behaving unpredictably instead
of just &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Fail-fast"&gt;failing fast&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let me elaborate based on the following code snippet. The function &lt;code&gt;approximate&lt;/code&gt;
takes two inputs &lt;code&gt;i1&lt;/code&gt; and &lt;code&gt;i2&lt;/code&gt; and one hyperparameter input &lt;code&gt;i3&lt;/code&gt;.
The latter is assumed to be a positive integer.
Local parameter &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;i3&lt;/code&gt;
need to be protected (e.g., using software redundancy) otherwise the loop might not terminate.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;approximate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;The question now is what shall we do with functions &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt;.
Leaving them unprotected means that we risk producing unpredictable value in &lt;code&gt;result&lt;/code&gt;.
Note that a control-flow decision at line #4 is based on them.
This should provide a good reason for protecting them.&lt;/p&gt;
&lt;p&gt;On the other hand, protecting &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; with software-based reliability
can prove more costly than running function &lt;code&gt;approximate&lt;/code&gt; on deterministic hardware in the first place.
This is the essence of the control-flow wall.&lt;/p&gt;
&lt;p&gt;Basically, decisions taken in the control-flow usually depend on the processed data.
Guaranteeing data correctness is costly to do in software.
On the other hand, allowing nondeterministic errors to affect data means that we can't, in general, guarantee how the algorithm would behave.&lt;/p&gt;
&lt;p&gt;General-purpose programming on nondeterministic hardware was tackled in Chisel &lt;a class="footnote-reference" href="#id29" id="id13"&gt;[13]&lt;/a&gt;
(A successor for a language called Rely).
The authors assume a hardware model where processors provide reliable and unreliable
versions of instructions. Also, data can be stored in an unreliable memory.&lt;/p&gt;
&lt;p&gt;There, developers are expected to provide reliability specifications.
Also, hardware engineers need to provide approximation specification.
The authors combine static analysis and Integer-Linear Programming in
order explore the design-space while maintaining the validity of reliability specification.
Still, their analyses are limited by data dependencies in the control-flow graph.&lt;/p&gt;
&lt;p&gt;It's important to differentiate between the reliability specification in Chisel
and the similar probabilistic specification of say Uncertain&amp;lt;T&amp;gt; &lt;a class="footnote-reference" href="#id30" id="id14"&gt;[14]&lt;/a&gt;.
The latter is a probabilistic programming extension to general-purpose languages.
This means that the inputs are, typically, prior probability distributions that need to
be processed &lt;em&gt;deterministically&lt;/em&gt; .&lt;/p&gt;
&lt;p&gt;That said, and without being able to guarantee (probable) correctness,
would it make any sense to use nondeterministic hardware? Well, it depends.
In the case where the cost of failure is small, and errors can't propagate deep into the program
anyway, being failure oblivious might make sense &lt;a class="footnote-reference" href="#id31" id="id15"&gt;[15]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, heterogeneous reliability, from my perspective, can make nondeterministic
hardware a viable option in practice.
Basically, reliable cores are used to run operating systems, language runtime, and programs.
Only compute-intensive kernels/regions might be offloaded to accelerators which are possibly built using unreliable nondeterministic hardware.
A notable example here is ERSA &lt;a class="footnote-reference" href="#id32" id="id16"&gt;[16]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Improving efficiency is a continuous endeavor in all engineering disciplines.
This endeavor requires balancing trade-offs between system cost and gained value.
The goal is to obtain results that are good enough for the cost we invest.&lt;/p&gt;
&lt;p&gt;Approximate computing is the research area where we attempt to realize techniques to
&lt;em&gt;automatically&lt;/em&gt; gain computing efficiency by trading off output quality with a metric of interest
such as performance and energy.
Automation is key to the value proposal of approximate computing as practitioners
are able of manually balancing those trade-offs already.&lt;/p&gt;
&lt;p&gt;Approximation also needs to be &lt;em&gt;principled&lt;/em&gt; which allow practitioners to
trust the system to behave as expected in the real world.
Combining automation and principled guarantees is essential, in my opinion,
for approximate computing to have a secure place in the engineering toolbox.&lt;/p&gt;
&lt;p&gt;This article briefly introduced approximate computing.
The discussion covered the entire system stack from algorithms to hardware circuits.
Also, I elaborated a bit on nondeterministic approximation computing due to
the special attention it received from the research community.&lt;/p&gt;
&lt;p&gt;&lt;span class="raw-html"&gt;&lt;hr/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id17" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[1]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Mittal, “A Survey of Techniques for Approximate Computing,” ACM Comput. Surv., vol. 48, no. 4, pp. 1–33, Mar. 2016.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id18" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[2]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;Q.&lt;/tt&gt; Xu, et al. “Approximate Computing: A Survey,” IEEE Des. Test, vol. 33, no. 1, pp. 8–22, Feb. 2016.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id19" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[3]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Chakradhar and A. Raghunathan, “Best-effort computing: Re-thinking Parallel Software and Hardware,” in Proceedings of the 47th Design Automation Conference (DAC ’10), 2010, p. 865.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id20" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[4]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;I.&lt;/tt&gt; Goiri, et al. “ApproxHadoop: Bringing Approximations to MapReduce Frameworks,” in Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS’15), 2015, vol. 43, no. 1, pp. 383–397.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id21" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[5]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;M.&lt;/tt&gt; Andrychowicz, et al., “Learning to learn by gradient descent by gradient descent,” preprint arXiv:1606.04474, 2016.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id22" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[6]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;X.&lt;/tt&gt; Sui, A. Lenharth, D. S. Fussell, and K. Pingali, “Proactive Control of Approximate Programs,” in Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’16), 2016, pp. 607–621.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id23" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[7]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Sidiroglou-Douskos, et al. “Managing performance vs. accuracy trade-offs with loop perforation,” in Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (FSE’11), 2011, pp. 124–134.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id24" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[8]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Venkataramani, et al. “Quality programmable vector processors for approximate computing,” in Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture - MICRO-46, 2013, pp. 1–12.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id25" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[9]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Venkataramani, et al. “SALSA: Systematic logic synthesis of approximate circuits,” in Proceedings of the 49th Annual Design Automation Conference on - DAC ’12, 2012, pp. 796–801.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id26" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[10]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;M.&lt;/tt&gt; Samadi, et al. “SAGE: self-tuning approximation for graphics engines,” in Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'46), 2013, pp. 13–24.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id27" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[11]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;M.&lt;/tt&gt; de Kruijf, et al. “Relax: an architectural framework for software recovery of hardware faults,” in Proceedings of the 37th annual international symposium on Computer architecture, 2010, pp. 497–508.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id28" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[12]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;E.&lt;/tt&gt; Cheng, et al. “CLEAR: Cross-Layer Exploration for Architecting Resilience: Combining Hardware and Software Techniques To Tolerate Soft Errors in Processor Cores,” in Proceedings of the 53rd Annual Design Automation Conference (DAC’16), 2016, p. 68.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id29" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[13]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;S.&lt;/tt&gt; Misailovic, et al. “Chisel: reliability- and accuracy-aware optimization of approximate computational kernels,” in Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages &amp;amp; Applications (OOPSLA’14), 2014, pp. 309–328.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id30" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[14]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;J.&lt;/tt&gt; Bornholt, et al. “Uncertain&amp;lt; T &amp;gt;: a first-order type for uncertain data,” in Proceedings of the 19th international conference on Architectural support for programming languages and operating systems, 2014, pp. 51–66.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id31" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[15]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;M.&lt;/tt&gt; Rinard, et al. “Enhancing server availability and security through failure-oblivious computing,” in Proceedings of the 6th conference on Symposium on Opearting Systems Design &amp;amp; Implementation (OSDI’04), 2004, p. 21.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id32" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[16]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;L.&lt;/tt&gt; Leem, et al. “ERSA: Error Resilient System Architecture for Probabilistic Applications,” in Proceedings of the Conference on Design, Automation and Test in Europe (DATE'10), 2010, pp. 1560–1565.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="research"></category><category term="approximate computing"></category><category term="reliability"></category></entry><entry><title>Undefined behavior in the wild</title><link href="https://blog.formallyapplied.com/2017/05/ub-in-wild/" rel="alternate"></link><published>2017-05-25T10:20:00+02:00</published><updated>2017-05-25T10:20:00+02:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2017-05-25:/2017/05/ub-in-wild/</id><summary type="html">&lt;p class="first last"&gt;So I was hacking an open source C++ project. The project builds
using gcc v5.4 and runs without problems. Then, I grabbed gcc v6.2 to see
how much performance improvement can be gained. Surprisingly, the program segfaulted ...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;So I was hacking the code of an open source C++ project. The project builds
using gcc v5.4 and runs without problems. Then, I grabbed gcc v6.2 to see
how much performance improvement can it bring especially with its more updated LTO.
Expectedly, the project builds without problems. However, it failed at run-time
with a segfault.&lt;/p&gt;
&lt;p&gt;Actually, such an error is usually a sign of &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Undefined_behavior"&gt;Undefined Behavior&lt;/a&gt; (UB)
constructs that exists in the program. Specifically, a newer compiler (version) might be stricter
by rejecting programs that are perfectly acceptable by an older compiler.
However, such rejection should happen at compile time and not by causing unexpected errors at run-time.&lt;/p&gt;
&lt;p&gt;UB is a known concept in C/C++ which refers to operations that have no defined semantics
in the language specifications.
Therefore, compilers do not expect such operations to happen.
Basically, UB specifications constitute a &amp;quot;contract&amp;quot; that the developer needs to maintain.
In the case of violating this contract, the compiler is basically free
to do whatever it sees fit. This includes summoning &lt;a class="reference external" href="https://en.wiktionary.org/wiki/nasal_demon"&gt;nasal demons&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the following, I shall elaborate on this UB instance since it involves several interesting
C++ concepts. My discussion, while focusing on a particular UB instance, can provide
insights on the problems caused by UB in your code and how to avoid them.
For more details on UB, I recommend John Regehr's &lt;a class="reference external" href="http://blog.regehr.org/archives/213"&gt;blog series&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="identifying-the-problem"&gt;
&lt;h2&gt;Identifying the problem&lt;/h2&gt;
&lt;p&gt;Consider the following code snippet which is a reduced version of the original UB instance.
The developer created the templated method &lt;code&gt;as&lt;/code&gt; (line 7) as a concise way to do
dynamic casting across the class hierarchy. The code &amp;quot;works&amp;quot; after compiling it
with gcc v5.4 or clang v4.8. Note that a nullptr check is in place (line 24) since
&lt;code&gt;snd_ptr&lt;/code&gt; is expected to be nullptr in case &lt;code&gt;fst_ptr&lt;/code&gt; was nullptr.
Interestingly, this snippet works even when compiled with gcc v6.2 with no optimizations (-O0).
However, it segfaults at line 22 when compiled with gcc v6.2 at optimization levels (-O1) or higher.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Base&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;dynamic_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;virtual&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Base&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;fst_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;nullptr&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;g++ 6.2 segfaults in the next statement&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;snd_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fst_ptr&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Other compilers continue ... &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;snd_ptr&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="k"&gt;nullptr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;snd_ptr&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Let's now analyze this case a bit further. The root cause of the problem is setting
&lt;code&gt;fst_ptr&lt;/code&gt; to &lt;code&gt;nullptr&lt;/code&gt;. However, this should not cause a problem locating
method &lt;code&gt;as&lt;/code&gt; since it's a non-virtual method.
Remember that the compiler can emit direct calls for non-virtual methods at compile-time.
However, in the case of virtual methods, it needs to locate the &lt;cite&gt;vtable&lt;/cite&gt; of the object
instance at run-time. This would not be possible if the pointer to the object was nullptr.&lt;/p&gt;
&lt;p&gt;Having checked that method &lt;code&gt;as&lt;/code&gt; is callable. We turn our attention to the dynamic
cast inside it (line 9). Passing a nullptr to &lt;code&gt;dynamic_cast&lt;/code&gt; is guaranteed
to return nullptr according to the C++ standard §5.2.7/4:
(thanks to this SO &lt;a class="reference external" href="https://stackoverflow.com/a/5155876"&gt;answer&lt;/a&gt;).
This suggests that in the case &lt;code&gt;this&lt;/code&gt; was nullptr, method &lt;code&gt;as&lt;/code&gt; must
return nullptr also. However, things are not that easy! Actually, setting &lt;code&gt;this&lt;/code&gt; to
nullptr is UB. It happens that gcc v6.2 did act upon this and produced a segfault
instead of calling &lt;code&gt;dynamic_cast&lt;/code&gt; on a nullptr as input.
Further, adding a check like &amp;quot;if (this == nullptr)&amp;quot; before dynamic casting
won't help either as compilers are free to optimize this check.&lt;/p&gt;
&lt;p&gt;So we detected such a toy example of UB using manual analysis. This manual method
might work with smaller codebases. However, it is unlikely that it can scale to larger codebases
or more complex cases of UB. Fortunately, reasoning about UB has been largely
automated in recent years thanks to the UB sanitizers integrated into gcc and clang.
For example, we can compile the above snippet using the following command,&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
g++ -std&lt;span class="o"&gt;=&lt;/span&gt;c++11 -Wall -Wextra -Wpedantic -fsanitize&lt;span class="o"&gt;=&lt;/span&gt;undefined ub.cpp -o ub.out
&lt;/pre&gt;
&lt;p&gt;Switching compiler flags to highest warning level is not critical here.
However, it is a recommended practice. Basically, we only need to pass the flag &lt;code&gt;-fsanitize=undefined&lt;/code&gt;
in order to activate the UB sanitizer.
Running the resulting executable &lt;code&gt;ub.out&lt;/code&gt;, would immediately produce suitable error messages
identifying the source of UB to be the call at line 22.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="possible-solutions"&gt;
&lt;h2&gt;Possible solutions&lt;/h2&gt;
&lt;p&gt;After detecting the UB source, the question is how to fix it?
Well, as discussed previously, it's the responsibility of developers to avoid
UB in their code. Ideally, this means to ensure that UB never happens for &lt;em&gt;any&lt;/em&gt; program input.
In practice, however, we need to avoid unnecessary checks by ensuring that UB does not happen
on &amp;quot;expected&amp;quot; inputs - at least.&lt;/p&gt;
&lt;p&gt;Translating this to our UB case, we can either (1) insert nullptr checks &lt;em&gt;before&lt;/em&gt; calling
method &lt;code&gt;as&lt;/code&gt;, (2) simply replace calls to method &lt;code&gt;as&lt;/code&gt; with dynamic casts,
or (3) convert method &lt;code&gt;as&lt;/code&gt; to be static. The last option requires the least amount
of change to the existing code which is a desirable property. Also,
it has the advantage of maintaining the type safety of the original code.
That is, method &lt;code&gt;as&lt;/code&gt; shall accept only class &lt;code&gt;Base&lt;/code&gt; or one of its derived classes as input.
The final code after applying solution (3) will look something like the following,&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Base&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Base&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;dynamic_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;virtual&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Base&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;fst_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;nullptr&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;snd_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Derived&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fst_ptr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;snd_ptr&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="k"&gt;nullptr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;snd_ptr&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Finally, building portable and future-proof software in C/C++ requires paying
attention to UB in your code. The availability of excellent tool support in the
form of UB sanitizers, among others, has made catching UB significantly easier.
It's recommended to regularly activate UB sanitizers on your code
in order to catch potential issues early and more often.&lt;/p&gt;
&lt;/div&gt;
</content><category term="programming"></category><category term="c++"></category><category term="gcc"></category><category term="undefined behavior"></category></entry><entry><title>Floating-point satisfiability as global optimization</title><link href="https://blog.formallyapplied.com/2017/05/gosat-faq/" rel="alternate"></link><published>2017-05-10T10:20:00+02:00</published><updated>2017-05-10T10:20:00+02:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2017-05-10:/2017/05/gosat-faq/</id><summary type="html">&lt;p class="first last"&gt;Solving SMT formulas involving the theory of floating-pointing arithmetic (FPA)
can be encountered in several domains including symbolic execution and program synthesis.
Unfortunately, SMT solvers which support FPA often struggle
trying to satisfy ...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Solving SMT formulas involving the theory of floating-pointing arithmetic (FPA)
can be required in several domains including symbolic execution, test generation,
and program synthesis. Unfortunately, SMT solvers often struggle
trying to satisfy given FPA queries. Additionally, commonly used non-linear functions,
e.g. trigonometric, remain unsupported.
This leaves tool developers with limited options like employing uninterpreted functions
which can be tricky or work around the issue by ignoring FPA altogether.&lt;/p&gt;
&lt;p&gt;Recently, I've been exploring &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Global_optimization"&gt;global optimization&lt;/a&gt;
as an alternative option for tackling this problem. This approach is unconventional
in the sense that it departs away from the known and trusted DPLL(T) framework.
This work is based on the ideas published in the &lt;a class="reference external" href="http://dx.doi.org/10.1007/978-3-319-41540-6_11"&gt;XSat&lt;/a&gt;
paper by Fu et al. (CAV'16). Our results are discussed in this &lt;a class="reference external" href="/docs/gosat.pdf"&gt;report&lt;/a&gt;.
This work is implemented in the SMT solver &lt;em&gt;goSAT&lt;/em&gt; which is publicly &lt;a class="reference external" href="https://github.com/abenkhadra/gosat"&gt;available&lt;/a&gt;.
We provide in the following an appendix to our report in Q&amp;amp;A format.
I find this format particularly useful for providing quick takeaways
to interested readers.
Additionally, it can help in clearing possible misunderstandings.&lt;/p&gt;
&lt;div class="section" id="appendix"&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;blockquote class="pull-quote"&gt;
&lt;strong&gt;Note&lt;/strong&gt;. This appendix was edited in collaboration with Dominik Stoffel and Wolfgang Kunz.
Errors and omissions are mine.&lt;/blockquote&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name" /&gt;
&lt;col class="field-body" /&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;Q:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;Evaluations results demonstrate that MathSat still compares favorably in terms of SAT instances solved.
Why do we need exploring alternatives to conventional solvers?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;A:&lt;/th&gt;&lt;td class="field-body"&gt;Our experimental evaluation considered only one set of FPA benchmarks.
Hence, it's still early to draw general conclusions.
Actually, our main target was assessing the potential of different GO
algorithms rather than comparing with MathSAT. Also, the merit of goSAT
is that it can, in principle, reason about any executable function, e.g., trigonometric
functions, which is not possible in MathSat. Further, extending MathSat to
Optimization-Modulo-Theory (OMT) over FPA will require implementing GO algorithms
similar to the ones we discussed. Finally, goSAT has a significant advantage in
terms of query time and used memory despite the fact that it's still at an early stage.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;Q:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;Wouldn't compiler optimizations affect the soundness of the results?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;A:&lt;/th&gt;&lt;td class="field-body"&gt;goSAT emits a restricted IR subset that doesn't have common features
like loops and heap allocations. Also, compiler optimizations are restricted to only one
function representing &lt;span class="math"&gt;\(\mathcal{G}(\vec{x})\)&lt;/span&gt;, i.e. intra-procedural optimizations only.
Works on compiler validation e.g. EMI &lt;a class="citation-reference" href="#pldi14" id="id1"&gt;[PLDI14]&lt;/a&gt; and CSmith &lt;a class="citation-reference" href="#pldi11" id="id2"&gt;[PLDI11]&lt;/a&gt;, demonstrate that
compiler &lt;em&gt;miscompilation&lt;/em&gt; bugs generally require code with more complex features.
Additionally, goSAT is incomplete when stochastic GO algorithms are applied.
However, it is sound w.r.t. SAT results.
SAT results are easy to validate externally using a simple evaluation.
We implemented this feature in goSAT and, so far, did not encounter invalid results.
Consequently, we do not consider potential compiler bugs to be a serious drawback.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;Q:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;What are the contributions of goSAT over XSat?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;A:&lt;/th&gt;&lt;td class="field-body"&gt;We provide several contributions over XSat.
First, XSat discussed one stochastic GO algorithm (MCMC sampling) and used Scipy as backend
in the experimentation. Scipy supports only one GO algorithm, namely, Basin Hopping.
In comparison, we provide a design-space exploration of different algorithms and backends, e.g., NLopt and IpOpt.
We evaluated four stochastic algorithms and one deterministic algorithm.
Second, our tool, goSAT, is publicly available while XSat is not public.
Third, with &lt;em&gt;BH solver&lt;/em&gt; we provide an open re-implementation of XSat with the same optimization parameters,
as provided to us by the XSat authors.
Fourth, goSAT is more portable compared to XSat since it uses CTypes for FFI instead of C extensions for Python.
Despite this, solving time of both tools is still comparable.
Fifth, we highlighted equations (9-11) which were overlooked in XSat despite being necessary for proving soundness.
Sixth, based on Figure 2, we made the observation that the regularity of generated functions is essential
to making goSAT work in practice.
Finally, XSat requires a manual setup to get working.
In contrast, goSAT is an integrated tool featuring JIT compilation of SMT formulas.
To our knowledge, we are the first to implement this in the context of SMT solving.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;Q:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;BH and NL solvers only work if given a compiled shared library as &amp;quot;input&amp;quot;.
One would typically expect an SMT formula to be given as input instead?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;A:&lt;/th&gt;&lt;td class="field-body"&gt;In code generation mode, goSAT generates C code (like XSat) that corresponds
to the given input formula(s). This code needs
to be compiled and given as input to BH or NL solvers. Additionally, goSAT can
solve SMT formulas directly. This is an extension over XSat and a key contribution.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;Q:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;Can goSAT provide stable results after multiple runs on the same formula?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;A:&lt;/th&gt;&lt;td class="field-body"&gt;With stochastic search, a certain degree of instability is unavoidable.
In our experiments, however, we experienced stable results with the majority
of input formulas. For example, applying &lt;code&gt;CRS2&lt;/code&gt; algorithm to the 214 instances in the griggio benchmarks,
we found only two instances, namely, &lt;code&gt;sqrt_c_2&lt;/code&gt; and &lt;code&gt;sqrt_c_5&lt;/code&gt;, to
lead to what can be considered unstable results.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class="raw-html"&gt;&lt;hr/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;!-- comment rubric:: References --&gt;
&lt;table class="docutils citation" frame="void" id="pldi14" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[PLDI14]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;V.&lt;/tt&gt; Le, M. Afshari, and Z. Su, “Compiler validation via equivalence modulo inputs,” in Proceedings of the 35th Conference on Programming Languages Design and Implementation (PLDI’14), 2014, pp. 216–226.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pldi11" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[PLDI11]&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;X.&lt;/tt&gt; Yang, Y. Chen, E. Eide, and J. Regehr, “Finding and understanding bugs in C compilers,” in Proceedings of the 32nd conference on Programming language design and implementation (PLDI’11), 2011, pp. 283–294.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="research"></category><category term="smt"></category><category term="floating-point"></category></entry><entry><title>Building this blog</title><link href="https://blog.formallyapplied.com/2017/05/building-this-blog/" rel="alternate"></link><published>2017-05-04T10:20:00+02:00</published><updated>2017-05-10T18:40:00+02:00</updated><author><name>Ammar Ben Khadra</name></author><id>tag:blog.formallyapplied.com,2017-05-04:/2017/05/building-this-blog/</id><summary type="html">&lt;p class="first last"&gt;Details about the tools and services used in building this blog. Also, some discussion
about the design rationale behind it.&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="in-a-nutshell"&gt;
&lt;h2&gt;In a nutshell&lt;/h2&gt;
&lt;p&gt;This blog is powered by &lt;a class="reference external" href="https://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; using
&lt;a class="reference external" href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3"&gt;pelican-bootstrap3&lt;/a&gt;
theme. It's hosted on &lt;a class="reference external" href="https://pages.github.com/"&gt;Github Pages&lt;/a&gt;.  Content distribution
and TLS encryption are powered by &lt;a class="reference external" href="https://www.cloudflare.com"&gt;CloudFlare&lt;/a&gt;.
In terms of cost, I had to pay only for the domain name registration.
A template of this blog is provided &lt;a class="reference external" href="https://github.com/abenkhadra/formallyapplied.template"&gt;here&lt;/a&gt;.
Basically, you only need to &lt;a class="reference external" href="http://docs.getpelican.com/en/3.7.1/install.html"&gt;install pelican&lt;/a&gt;
and you should be able of getting the template up and running in a matter of minutes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="design-options"&gt;
&lt;h2&gt;Design options&lt;/h2&gt;
&lt;p&gt;If you are interested in the alternative options considered when building this
blog then please read on.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Static or dynamic blog&lt;/strong&gt;. This decision effectively means choosing between
&lt;a class="reference external" href="https://www.wordpress.com"&gt;Wordpress&lt;/a&gt; and a Static Site Generator (SSG). I won't
delve into a detailed comparison here. Quick googling should give you a sufficient idea.
Bottom line is that if you have a limited set of requirements - similar to my case -
then I recommend going for a static website.
Basically, it allows you to focus on your content and largely forget about
issues related to maintenance, security, and performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Choosing an SSG&lt;/strong&gt;. There are plenty of SSGs around on &lt;a class="reference external" href="https://www.staticgen.com"&gt;staticgen&lt;/a&gt;.
What ended up on my shortlist are Jekyll, Hugo, and Pelican. Jekyll is unquestionably
the most popular and is the one endorsed by GitHub. Also,
I read some nice things about Hugo particularly regarding its speed. However, I haven't
tried any of them, to be honest, and went directly for Pelican due to these reasons;
First, it is written in Python. I'm learning Python already and this helps
me have more consolidation around its ecosystem. Second, Pelican comes with native
support for restructuredText which has several
&lt;a class="reference external" href="http://eli.thegreenplace.net/2017/restructuredtext-vs-markdown-for-technical-documentation/"&gt;advantages&lt;/a&gt;
over Markdown.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blog theme&lt;/strong&gt;. Fortunately, Pelican has a lot of &lt;a class="reference external" href="http://www.pelicanthemes.com/"&gt;themes&lt;/a&gt;
to choose from. I ended up choosing &lt;em&gt;pelican-bootstrap3&lt;/em&gt;. This theme supports
Booststrap (that is easy to guess!) and its corresponding &lt;a class="reference external" href="https://bootswatch.com/"&gt;bootswatch&lt;/a&gt; themes.
Then, I customized my own bootswatch sub-theme which is named &amp;quot;formally&amp;quot; based on the Sandstone sub-theme.
In this regard, I tried to follow some rules like the ones found
&lt;a class="reference external" href="https://spark.adobe.com/blog/2017/01/30/13-rules-to-help-you-stop-making-bad-font-choices/"&gt;here&lt;/a&gt;
and &lt;a class="reference external" href="https://designschool.canva.com/blog/design-rules//"&gt;here&lt;/a&gt;.
These rules seem to summarize sensible wisdom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, in order to apply your own customization to the provided template,
consider using &lt;a class="reference external" href="https://developers.google.com/web/tools/setup/setup-workflow"&gt;Map to Network Resource&lt;/a&gt;
which is a feature available in Chrome. This feature helps
you iterate quickly and thus saves you a lot of time in the process&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="blogging"></category></entry></feed>